
Setting a User-Agent can sometimes get around an access denied error (like the [[5a. Forbidden access to sitemap|403 Forbidden error when trying to access a sitemap]]) because many web servers use the User-Agent string to determine the type of the requesting entity, often to differentiate between requests made by a human using a web browser and those made by bots or automated scripts.

Some sites are configured to only allow access to requests that come from common web browsers. So by setting a User-Agent string that mimics a popular browser, the HTTP request to access a sitemap may be treated as though it was made by a user browsing the web.

The following code tries to access a sitemap without setting a User-Agent. If a 403 status code is returned (forbidden access), a new request is sent with a User-Agent set to mimic a browser.

This worked to get around the forbidden error I got when trying to access the sitemap for. "https://www.prpimaging.com.au/wpsl_stores-sitemap.xml"

```python
def get_sitemap(sitemap_url):  
  sitemap = requests.get(sitemap_url)
    
  if sitemap.status_code == 403:  
    print("403 Forbidden access to sitemap, trying with headers")  
    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}  
    return requests.get(sitemap_url, headers=headers).content 
     
  return sitemap.content  

print(get_sitemap('https://www.prpimaging.com.au/wpsl_stores-sitemap.xml'))
```

---
[[6. Get html as soup object from url]] - Also sets user agent to get access to html via url.