#litnote
---
Keepsies

Purpose: How to generate a book summary that is as good or better than a human can do.


- Evidence: Pu et al found in 2023 that summaries generated by LLMs are preferred over those written by humans (Xiao Pu, Mingqi Gao, and Xiaojun Wan. Summarization is (almost) dead, 2023b.)
	- Read this paper because that's a critical point and I want to know the reasons why they were preferred by those written by humans.

- Definition: Book-length documents are texts that are longer than 100k tokens (pp.1).
- Issue: Book-length documents (100k and larger) exceed the context window limits of today's LLMs (8k tokens for GPT-3.5 and GPT-4).
	- Consequently, we need to split the text into smaller pieces that fit within the context window (chunking).
	- Consequently, we need to process each chunk (summarise them).
	- Consequently, we need to combine and compress each chunk to create a summary of the whole book.
	- (Wu et al, 2021: Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris- tiano. Recursively summarizing books with human feedback, 2021.)
- What happens when the context window expands? Yesterday for example (Nov 6 2023), OpenAI released a statement that they were going to increase the context window from 8k to 120k tokens, which would cover most book-length documents. Claude is already close to that.

---
Chang, Y., Lo, K., Goyal, T., & Iyyer, M. (2023). BooookScore: A systematic exploration of book-length summarization in the era of LLMs. arXiv:2310.00785 [cs.CL]. [https://doi.org/10.48550/arXiv.2310.00785](https://doi.org/10.48550/arXiv.2310.00785)