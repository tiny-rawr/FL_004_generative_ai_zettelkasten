According to Chang et al 2023, the reason we need to chunk long documents in the first place is because LLMs have a limited number of input and output tokens they can process (8k-32k), which is far shorter than the typical length of a book (>100k).

Yesterday (Nov 6 2023), OpenAI announced that they were expanding the context window of their GPT models to 128k which is long enough to process most book-length documents. Other LLMs like Claud2, already has a context window of 100k.

So my question is, if the context window becomes large enough to accept any-sized document, will we still need to chunk them in order to produce high-quality book summaries?

---
[[3a1. Chunking helps prevent info overload in RAG]]