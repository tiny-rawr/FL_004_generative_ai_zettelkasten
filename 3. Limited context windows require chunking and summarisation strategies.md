Most LLMs have a limit to how much input and output tokens they can process, which is known as the context window. Tokens are the basic unit that LLMs use to determine the length of text, where each token can represent one or more characters.

OpenAI's GPT-3.5 and GPT-4 models have a context length of 8k, 16k and 32k tokens which are much smaller than the typical token length of a book, which tend to be around 100k tokens in length. Not only that, but the token limit includes both the input passed into the LLM, and the generated output.

Because the context window is much smaller than a book-length document, we need to split the book into smaller chunks of text in order to be able to pass it to the LLM for summarising. We also need to think about how to merge the chunk summaries in order to create a full summary of our book, which is likely to also end up being larger than the context window size.

The strategies we choose to chunk and summarise our book, should be chosen with the aim of [[1. Booookscore - A systematic exploration of book-length summarization in the era of LLMs |generating a book summary that is the same or better than a human could do manually]].

---
[[3a. Will chunking still be necessary for bigger context windows?]]