The following code fetches the HTML content from a URL. The HTML can then be scraped with BeautifulSoup. A User-Agent is set to mimic a web browser before requesting the HTML from a URL, to get around the 403 Forbidden error that some websites enforce. 

```python
from urllib.request import Request, urlopen  
from urllib.error import HTTPError, URLError  
from bs4 import BeautifulSoup  
  
def fetch_html(url):  
  try:  
    req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'})  
    return BeautifulSoup(urlopen(req), 'html.parser').select_one('html')  
  except HTTPError as e:  
    print(f"HTTP error: {e.code}")  
  except URLError as e:  
    print(f"URL error: {e.reason}")  
  
if __name__ == "__main__":  
  url = 'https://www.prpimaging.com.au/practices/central-coast/woy-woy/'  
  html = fetch_html(url)  
  
  print(html)
```

---
[[5a1. Set User Agent to get around 403 forbidden access to sitemap error]]